{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias of Gradient in Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in if the gradient of a mini-batch in SGD is biased, compared to the full gradient of the dataset. Specifically, $\\forall \\theta \\in \\Theta$, does\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} E( \\nabla L(X_m, Y_m, \\theta) ) = \\frac{1}{n} \\nabla L(X_n, Y_n, \\Theta)?\n",
    "\\end{equation}\n",
    "\n",
    "We try to answer this computationally since it may be hard to to analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We adapt the data from \"one dimension, three parameters\" from Li."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Simulation specifications\n",
    "N = 200\n",
    "sigma_sq = 0.1**2\n",
    "outputscale = 4**2\n",
    "lengthscale = 2\n",
    "\n",
    "# Features\n",
    "X = torch.linspace(0, 100, N)\n",
    "# Targets\n",
    "data_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "data_kernel.outputscale = outputscale\n",
    "data_kernel.base_kernel.lengthscale = lengthscale\n",
    "Sigma = (data_kernel(X, X) + torch.diag(torch.Tensor([sigma_sq for _ in X]))).detach().numpy()\n",
    "Y = torch.Tensor(np.random.multivariate_normal(np.zeros(N), Sigma, size = 1).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Model From GPyTorch Regression Tutorial\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X, Y, likelihood)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)  # Loss\n",
    "optimizer = torch.optim.Optimizer(model.parameters(), {})          # For zero-ing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions of $\\nabla L_m$ for a few $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters to examine\n",
    "model.likelihood.noise_covar.noise = 0.2**2\n",
    "model.covar_module.outputscale = 3**2\n",
    "model.covar_module.base_kernel.lengthscale = 1\n",
    "\n",
    "_ = model.eval()\n",
    "_ = likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means equal is True.\n",
      "Variances equal is True.\n",
      "However, mll_a is -0.32908713817596436 while mll_b is -0.3363455832004547.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:247: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "with gpytorch.settings.fast_pred_var():\n",
    "    a = model(X)\n",
    "    b = model(X)\n",
    "    print(f'Means equal is {torch.all(a.mean == b.mean)}.')\n",
    "    print(f'Variances equal is {torch.all(a.stddev == b.stddev)}.')\n",
    "    print(f'However, mll_a is {-mll(a, Y)} while mll_b is {-mll(b, Y)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = -0.3363158404827118 Full Gradient: tensor([ 0.2651,  0.0486, -4.9920,  3.0355])\n",
      "Loss = -0.34093788266181946 Full Gradient: tensor([ 0.2485,  0.0486, -5.3676, -0.8328])\n",
      "Loss = -0.3343978226184845 Full Gradient: tensor([ 0.2679,  0.0486, -5.2922,  1.9855])\n",
      "Loss = -0.3281000554561615 Full Gradient: tensor([ 0.2620,  0.0486, -5.4516,  1.1259])\n",
      "Loss = -0.3311139643192291 Full Gradient: tensor([ 0.2537,  0.0486, -5.3967, -1.6832])\n",
      "Loss = -0.336770236492157 Full Gradient: tensor([ 0.2692,  0.0486, -5.4962, -1.0641])\n",
      "Loss = -0.3502829670906067 Full Gradient: tensor([ 0.2532,  0.0486, -5.1526, -0.7554])\n",
      "Loss = -0.3273807466030121 Full Gradient: tensor([ 0.2615,  0.0486, -6.2553, -0.5962])\n",
      "Loss = -0.3370800018310547 Full Gradient: tensor([ 0.2562,  0.0486, -5.6861,  1.1181])\n",
      "Loss = -0.3353101313114166 Full Gradient: tensor([ 0.2648,  0.0486, -4.8926,  1.5607])\n"
     ]
    }
   ],
   "source": [
    "# Getting the true gradient\n",
    "for _ in range(10):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = -mll(preds, Y)\n",
    "        loss.backward()\n",
    "    nL = torch.cat([torch.flatten(param.grad) for _, param in model.named_parameters()])\n",
    "    print(f'Loss = {loss} Full Gradient: {nL}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last entry of the gradient behaves especially wildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Gradient: tensor([ 0.1904,  0.1489, -4.8787, -2.8360])\n"
     ]
    }
   ],
   "source": [
    "# Getting a minibatch gradient\n",
    "m = 10\n",
    "minibatch = np.random.choice(X.shape[0], m, replace = False)\n",
    "\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X[minibatch])\n",
    "    loss = -mll(output, Y[minibatch])\n",
    "    loss.backward()\n",
    "    \n",
    "nLm = torch.cat([torch.flatten(param.grad) for _, param in model.named_parameters()])\n",
    "print(f'Minibatch Gradient: {nLm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
