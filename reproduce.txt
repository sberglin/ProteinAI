import gpytorch
import torch
import numpy as np
import random

random.seed(123)
np.random.seed(123)

#### Data ####
# Simulation specifications
N = 200
sigma_sq = 0.1**2
outputscale = 4**2
lengthscale = 2
# Features
X = torch.linspace(0, 100, N)
# Targets
data_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
data_kernel.outputscale = outputscale
data_kernel.base_kernel.lengthscale = lengthscale
Sigma = (data_kernel(X, X) + torch.diag(torch.Tensor([sigma_sq for _ in X]))).detach().numpy()
Y = torch.Tensor(np.random.multivariate_normal(np.zeros(N), Sigma, size = 1).ravel())

# Simple Model From GPyTorch Regression Tutorial
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# Creating model
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(X, Y, likelihood)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)  # Loss
optimizer = torch.optim.Optimizer(model.parameters(), {})          # For zero-ing gradients

# Setting parameters to examine
model.likelihood.noise_covar.noise = 0.2**2
model.covar_module.outputscale = 3**2
model.covar_module.base_kernel.lengthscale = 1

_ = model.eval()
_ = likelihood.eval()

with gpytorch.settings.fast_pred_var():
    a = model(X)
    b = model(X)
    print(f'Means equal is {torch.all(a.mean == b.mean)}.')
    print(f'Variances equal is {torch.all(a.stddev == b.stddev)}.')
    print(f'However, mll_a is {-mll(a, Y)} while mll_b is {-mll(b, Y)}.')
    
# Means equal is True.
# Variances equal is True.
# However, mll_a is -0.32908713817596436 while mll_b is -0.3363455832004547.

# Getting the true gradient
for _ in range(10):
    with gpytorch.settings.fast_pred_var():
        optimizer.zero_grad()
        preds = model(X)
        loss = -mll(preds, Y)
        loss.backward()
    nL = torch.cat([torch.flatten(param.grad) for _, param in model.named_parameters()])
    print(f'Loss = {loss} Full Gradient: {nL}')  

# Loss = -0.3363158404827118 Full Gradient: tensor([ 0.2651,  0.0486, -4.9920,  3.0355])
# Loss = -0.34093788266181946 Full Gradient: tensor([ 0.2485,  0.0486, -5.3676, -0.8328])
# Loss = -0.3343978226184845 Full Gradient: tensor([ 0.2679,  0.0486, -5.2922,  1.9855])
# Loss = -0.3281000554561615 Full Gradient: tensor([ 0.2620,  0.0486, -5.4516,  1.1259])
# Loss = -0.3311139643192291 Full Gradient: tensor([ 0.2537,  0.0486, -5.3967, -1.6832])
# Loss = -0.336770236492157 Full Gradient: tensor([ 0.2692,  0.0486, -5.4962, -1.0641])
# Loss = -0.3502829670906067 Full Gradient: tensor([ 0.2532,  0.0486, -5.1526, -0.7554])
# Loss = -0.3273807466030121 Full Gradient: tensor([ 0.2615,  0.0486, -6.2553, -0.5962])
# Loss = -0.3370800018310547 Full Gradient: tensor([ 0.2562,  0.0486, -5.6861,  1.1181])
# Loss = -0.3353101313114166 Full Gradient: tensor([ 0.2648,  0.0486, -4.8926,  1.5607])
# The last entry of the gradient behaves especially wildly
    